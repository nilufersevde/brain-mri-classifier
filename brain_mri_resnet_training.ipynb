{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil \n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "train_dir = Path(\"data/brain_tumor_dataset/Training\")\n",
    "test_dir = Path(\"data/brain_tumor_dataset/Testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pituitary: 1457 images\n",
      ".DS_Store: 0 images\n",
      "notumor: 1595 images\n",
      "glioma: 1321 images\n",
      "meningioma: 1339 images\n"
     ]
    }
   ],
   "source": [
    "for class_folder in train_dir.iterdir():\n",
    "    num_images = len(list(class_folder.glob(\"*.jpg\")))\n",
    "    print(f\"{class_folder.name}: {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pituitary: 300 images\n",
      "notumor: 405 images\n",
      "glioma: 300 images\n",
      "meningioma: 306 images\n"
     ]
    }
   ],
   "source": [
    "for class_folder in test_dir.iterdir():\n",
    "    num_images = len(list(class_folder.glob(\"*.jpg\")))\n",
    "    print(f\"{class_folder.name}: {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "source_dir = Path(\"data/brain_tumor_dataset/Training\")\n",
    "target_dir = Path(\"data/clean_split\")\n",
    "\n",
    "train_target = target_dir / \"train\"\n",
    "val_target = target_dir / \"val\"\n",
    "\n",
    "train_target.mkdir(parents=True, exist_ok=True)\n",
    "val_target.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_folder in source_dir.iterdir():\n",
    "    if class_folder.is_dir():\n",
    "        train_class_dir = train_target / class_folder.name\n",
    "        val_class_dir = val_target / class_folder.name\n",
    "        \n",
    "        train_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "        val_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "         # List all images\n",
    "        image_paths = list(class_folder.glob(\"*.jpg\"))\n",
    "        \n",
    "        # Shuffle\n",
    "        random.shuffle(image_paths)\n",
    "        \n",
    "        # Split 80/20\n",
    "        split_idx = int(len(image_paths) * 0.8)\n",
    "        train_images = image_paths[:split_idx]\n",
    "        val_images = image_paths[split_idx:]\n",
    "        \n",
    "        for img_path in train_images:\n",
    "            shutil.copy(img_path, train_class_dir / img_path.name)\n",
    "        \n",
    "        for img_path in val_images:\n",
    "            shutil.copy(img_path, val_class_dir / img_path.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_source_dir = Path(\"data/brain_tumor_dataset/Testing\")\n",
    "test_target_dir = Path(\"data/clean_split/test\")\n",
    "\n",
    "for class_folder in test_source_dir.iterdir():\n",
    "    if class_folder.is_dir():\n",
    "        target_class_dir = test_target_dir / class_folder.name\n",
    "        target_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for img_path in class_folder.glob(\"*.jpg\"):\n",
    "            shutil.copy(img_path, target_class_dir / img_path.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root=\"data/clean_split/train\", transform=transform)\n",
    "val_dataset = ImageFolder(root=\"data/clean_split/val\", transform=transform)\n",
    "test_dataset = ImageFolder(root=\"data/clean_split/test\", transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nilufersevdeozdemir/Desktop/compmedprojects/brain-mri-classifier/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nilufersevdeozdemir/Desktop/compmedprojects/brain-mri-classifier/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Linear(in_features=512, out_features=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "                # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_epoch_loss = val_loss / len(val_loader)\n",
    "        val_epoch_acc = 100 * val_correct / val_total\n",
    "\n",
    "        print(f\"Validation Loss: {val_epoch_loss:.4f} Accuracy: {val_epoch_acc:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # In case images are grayscale but ResNet expects 3 channels\n",
    "    transforms.Resize((224, 224)),                # Resize to fit ResNet18 input size\n",
    "    transforms.ToTensor(),                        # Convert PIL image â†’ Tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 0.4226 Accuracy: 84.95%\n",
      "Validation Loss: 0.3950 Accuracy: 85.62%\n",
      "Epoch [2/5] Loss: 0.2140 Accuracy: 92.29%\n",
      "Validation Loss: 0.1251 Accuracy: 95.03%\n",
      "Epoch [3/5] Loss: 0.1597 Accuracy: 94.54%\n",
      "Validation Loss: 0.1294 Accuracy: 95.46%\n",
      "Epoch [4/5] Loss: 0.1354 Accuracy: 95.32%\n",
      "Validation Loss: 0.0745 Accuracy: 97.73%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train_model(resnet, train_loader, val_loader, criterion, optimizer, device, num_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
